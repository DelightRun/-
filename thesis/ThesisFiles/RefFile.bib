%% Created using Papers on Wed, 18 May 2016.
%% http://papersapp.com/papers/

@article{Gou:2015du,
author = {Gou, Chao and Wang, Kunfeng and Yao, Yanjie and Li, Zhengxi},
title = {{Vehicle License Plate Recognition Based on Extremal Regions and Restricted Boltzmann Machines}},
journal = {IEEE Transactions on Intelligent Transportation Systems},
year = {2015},
volume = {PP},
number = {99},
pages = {1--12},
publisher = {IEEE},
doi = {10.1109/TITS.2015.2496545},
read = {Yes},
rating = {0},
date-added = {2016-03-01T06:04:55GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {This paper presents a vehicle license plate recognition method based on character-specific extremal regions (ERs) and hybrid discriminative restricted Boltzmann machines (HDRBMs). First, coarse license plate detection (LPD) is performed by top-hat transformation, vertical edge detection, morphological operations, and various validations. Then, character-specific ERs are extracted as character regions in license plate candidates. Followed by suitable selection of ERs, the segmentation of characters and coarse-to-fine LPD are achieved simultaneously. Finally, an offline trained pattern classifier of HDRBM is applied to recognize the characters. The proposed method is robust to illumination changes and weather conditions during 24 h or one day. Experimental results on thorough data sets are reported to demonstrate the effectiveness of the proposed approach in complex traffic environments},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7331292},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/21/21EC3C3C-FBF4-406A-8EF9-8C6E6449BDAF.pdf},
file = {{21EC3C3C-FBF4-406A-8EF9-8C6E6449BDAF.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/21/21EC3C3C-FBF4-406A-8EF9-8C6E6449BDAF.pdf:application/pdf}},
uri = {\url{papers3://publication/doi/10.1109/TITS.2015.2496545}}
}

@article{Du:2013js,
author = {Du, Shan and Ibrahim, Mahmoud and Shehata, Mohamed and Badawy, Wael},
title = {{Automatic License Plate Recognition (ALPR): A State-of-the-Art Review}},
journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
year = {2013},
volume = {23},
number = {2},
pages = {311--325},
publisher = {IEEE},
doi = {10.1109/TCSVT.2012.2203741},
language = {English},
read = {Yes},
rating = {0},
date-added = {2016-02-23T07:00:39GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {Automatic license plate recognition (ALPR) is the extraction of vehicle license plate information from an image or a sequence of images. The extracted information can be used with or without a database in many applications, such as electronic payment systems (toll payment, parking fee payment), and freeway and arterial monitoring systems for traffic surveillance. The ALPR uses either a color, black and white, or infrared camera to take images. The quality of the acquired images is a major factor in the success of the ALPR. ALPR as a real-life application has to quickly and successfully process license plates under different environmental conditions, such as indoors, outdoors, day or night time. It should also be generalized to process license plates from different nations, provinces, or states. These plates usually contain different colors, are written in different languages, and use different fonts; some plates may have a single color background and others have background images. The license plates can be partially occluded by dirt, lighting, and towing accessories on the car. In this paper, we present a comprehensive review of the state-of-the-art techniques for ALPR. We categorize different ALPR techniques according to the features they used for each stage, and compare them in terms of pros, cons, recognition accuracy, and processing speed. Future forecasts of ALPR are given at the end.},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6213519},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/43/43A72772-D23E-42B3-A1EA-1A71A70E579D.pdf},
file = {{43A72772-D23E-42B3-A1EA-1A71A70E579D.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/43/43A72772-D23E-42B3-A1EA-1A71A70E579D.pdf:application/pdf;43A72772-D23E-42B3-A1EA-1A71A70E579D.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/43/43A72772-D23E-42B3-A1EA-1A71A70E579D.pdf:application/pdf}},
uri = {\url{papers3://publication/doi/10.1109/TCSVT.2012.2203741}}
}

@article{Baya:2008ud,
author = {Baya, H and Essa, A and Tuytelaarsb, T},
title = {{Speeded-up robust features (SURF)}},
journal = {Computer Vision and {\ldots}},
year = {2008},
rating = {0},
date-added = {2016-05-14T10:16:58GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {Abstract This article presents a novel scale-and rotation-invariant detector and descriptor, coined SURF ( Speeded - Up Robust Features ). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and ... 
},
url = {http://www.cs.jhu.edu/~misha/ReadingSeminar/Papers/Bay08.pdf},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/FA/FA239331-92FF-4EFA-9C0B-E03CE05D3A46.pdf},
file = {{FA239331-92FF-4EFA-9C0B-E03CE05D3A46.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/FA/FA239331-92FF-4EFA-9C0B-E03CE05D3A46.pdf:application/pdf}},
uri = {\url{papers3://publication/uuid/C8D29280-BEE8-48CC-BF35-3E75164337F0}}
}

@article{Lowe:2004kp,
author = {Lowe, David G},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
journal = {International Journal of Computer Vision},
year = {2004},
volume = {60},
number = {2},
pages = {91--110},
publisher = {Kluwer Academic Publishers},
doi = {10.1023/B:VISI.0000029664.99615.94},
language = {English},
rating = {0},
date-added = {2016-05-14T10:15:03GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {Abstract This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide ...
},
url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
uri = {\url{papers3://publication/doi/10.1023/B:VISI.0000029664.99615.94}}
}

@article{He:2016tq,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {{Identity Mappings in Deep Residual Networks}},
journal = {arXiv.org},
year = {2016},
eprint = {1603.05027v2},
eprinttype = {arxiv},
eprintclass = {cs.CV},
month = mar,
read = {Yes},
rating = {0},
date-added = {2016-04-13T04:00:53GMT},
date-modified = {2016-05-18T12:51:00GMT},
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which further makes training easy and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.},
url = {http://arxiv.org/abs/1603.05027v2},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/B4/B43C9808-6EA6-495A-8688-8B335C556190.pdf},
file = {{B43C9808-6EA6-495A-8688-8B335C556190.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/B4/B43C9808-6EA6-495A-8688-8B335C556190.pdf:application/pdf;B43C9808-6EA6-495A-8688-8B335C556190.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/B4/B43C9808-6EA6-495A-8688-8B335C556190.pdf:application/pdf}},
uri = {\url{papers3://publication/uuid/DEC8F855-C704-4F31-A9F3-B1AF3B63C087}}
}

@article{Gomez:2014vp,
author = {Gomez, Lluis and Karatzas, Dimosthenis},
title = {{A Fast Hierarchical Method for Multi-script and Arbitrary Oriented Scene Text Extraction}},
year = {2014},
eprint = {1407.7504},
eprinttype = {arxiv},
month = jul,
read = {Yes},
rating = {0},
date-added = {2016-04-28T05:36:33GMT},
date-modified = {2016-05-18T12:40:41GMT},
url = {http://arxiv.org/abs/1407.7504},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/01/011AB9B1-95BC-4EB6-8D1F-B5AC2DC335DB.pdf},
file = {{011AB9B1-95BC-4EB6-8D1F-B5AC2DC335DB.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/01/011AB9B1-95BC-4EB6-8D1F-B5AC2DC335DB.pdf:application/pdf;011AB9B1-95BC-4EB6-8D1F-B5AC2DC335DB.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/01/011AB9B1-95BC-4EB6-8D1F-B5AC2DC335DB.pdf:application/pdf}},
uri = {\url{papers3://publication/uuid/8B8F3FDF-081D-4321-A63B-67113CF4C137}}
}

@article{Zeiler:2012uw,
author = {Zeiler, Matthew D},
title = {{ADADELTA: An Adaptive Learning Rate Method}},
journal = {arXiv.org},
year = {2012},
eprint = {1212.5701v1},
eprinttype = {arxiv},
eprintclass = {cs.LG},
month = dec,
read = {Yes},
rating = {0},
date-added = {2016-02-06T10:17:53GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
url = {http://arxiv.org/abs/1212.5701v1},
uri = {\url{papers3://publication/uuid/1D1617CA-4BC4-4754-B074-47E1E3102263}}
}

@article{Nair:2010vq,
author = {Nair, Vinod and Hinton, Geoffrey E},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines.}},
journal = {ICML},
year = {2010},
pages = {807--814},
read = {Yes},
rating = {0},
date-added = {2016-01-25T13:54:05GMT},
date-modified = {2016-05-18T12:40:41GMT},
url = {http://www.icml2010.org/papers/432.pdf},
uri = {\url{papers3://publication/uuid/03C85B19-6C1C-4075-8D36-7CD321E02ECE}}
}

@article{LeCun:1990vp,
author = {Le Cun, B B and Denker, J S and Henderson, D},
title = {{Handwritten digit recognition with a back-propagation network}},
journal = {Advances in neural {\ldots}},
year = {1990},
read = {Yes},
rating = {0},
date-added = {2015-11-27T11:17:05GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {Abstract We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network ...
},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.5076},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/61/610E9F05-F924-436E-A4BB-9A8EDF4D61DC.pdf},
file = {{610E9F05-F924-436E-A4BB-9A8EDF4D61DC.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/61/610E9F05-F924-436E-A4BB-9A8EDF4D61DC.pdf:application/pdf;610E9F05-F924-436E-A4BB-9A8EDF4D61DC.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/61/610E9F05-F924-436E-A4BB-9A8EDF4D61DC.pdf:application/pdf}},
uri = {\url{papers3://publication/uuid/96CF6C4E-96A8-4B5F-98FA-60C161D4F14F}}
}

@article{Girshick:2015ib,
author = {Girshick, Ross B},
title = {{Fast R-CNN.}},
journal = {ICCV},
year = {2015},
pages = {1440--1448},
publisher = {IEEE},
doi = {10.1109/ICCV.2015.169},
isbn = {978-1-4673-8391-2},
read = {Yes},
rating = {0},
date-added = {2016-04-09T13:08:38GMT},
date-modified = {2016-05-18T12:40:41GMT},
url = {http://dx.doi.org/10.1109/ICCV.2015.169},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/EF/EF536C38-9190-44EF-8DC5-4CFAEC10365C.pdf},
file = {{EF536C38-9190-44EF-8DC5-4CFAEC10365C.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/EF/EF536C38-9190-44EF-8DC5-4CFAEC10365C.pdf:application/pdf;EF536C38-9190-44EF-8DC5-4CFAEC10365C.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/EF/EF536C38-9190-44EF-8DC5-4CFAEC10365C.pdf:application/pdf}},
uri = {\url{papers3://publication/doi/10.1109/ICCV.2015.169}}
}

@article{Krizhevsky:2012wl,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
title = {{ImageNet Classification with Deep Convolutional Neural Networks.}},
journal = {NIPS},
year = {2012},
pages = {1106--1114},
read = {Yes},
rating = {0},
date-added = {2016-01-18T06:53:53GMT},
date-modified = {2016-05-18T12:40:41GMT},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/F8/F89BA960-BCFB-412D-96E4-75D49ACDE10E.pdf},
file = {{F89BA960-BCFB-412D-96E4-75D49ACDE10E.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/F8/F89BA960-BCFB-412D-96E4-75D49ACDE10E.pdf:application/pdf;F89BA960-BCFB-412D-96E4-75D49ACDE10E.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/F8/F89BA960-BCFB-412D-96E4-75D49ACDE10E.pdf:application/pdf}},
uri = {\url{papers3://publication/uuid/0219E9A1-2F03-4806-8264-96E1D9EEC3D9}}
}

@article{Duda:1972eua,
author = {Duda, Richard O and Hart, Peter E},
title = {{Use of the Hough transformation to detect lines and curves in pictures}},
journal = {Communications of the ACM},
year = {1972},
volume = {15},
number = {1},
pages = {11--15},
month = jan,
publisher = {ACM},
doi = {10.1145/361237.361242},
read = {Yes},
rating = {0},
date-added = {2016-05-14T10:12:21GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {Abstract Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. It also shows how the ...
},
url = {http://portal.acm.org/citation.cfm?doid=361237.361242},
uri = {\url{papers3://publication/doi/10.1145/361237.361242}}
}

@article{McCulloch:1943dq,
author = {McCulloch, Warren S and Pitts, Walter},
title = {{A logical calculus of the ideas immanent in nervous activity}},
journal = {The bulletin of mathematical biophysics},
year = {1943},
volume = {5},
number = {4},
pages = {115--133},
publisher = {Kluwer Academic Publishers},
doi = {10.1007/BF02478259},
language = {English},
read = {Yes},
rating = {0},
date-added = {2016-05-16T07:06:29GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {Abstract Because of the {\textquotedblleft}all-or-none{\textquotedblright} character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more ...
},
url = {http://link.springer.com/10.1007/BF02478259},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/33/330CBD64-87B0-49B5-8FBB-0C20E5AD816A.pdf},
file = {{330CBD64-87B0-49B5-8FBB-0C20E5AD816A.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/33/330CBD64-87B0-49B5-8FBB-0C20E5AD816A.pdf:application/pdf;330CBD64-87B0-49B5-8FBB-0C20E5AD816A.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/33/330CBD64-87B0-49B5-8FBB-0C20E5AD816A.pdf:application/pdf}},
uri = {\url{papers3://publication/doi/10.1007/BF02478259}}
}

@article{Rosenblatt:1958jc,
author = {Rosenblatt, F},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain.}},
journal = {Psychological Review},
year = {1958},
volume = {65},
number = {6},
pages = {386--408},
month = nov,
publisher = {American Psychological Association},
doi = {10.1037/h0042519},
language = {English},
read = {Yes},
rating = {0},
date-added = {2016-05-16T07:06:49GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
url = {http://psycnet.apa.org/journals/rev/65/6/386.html},
uri = {\url{papers3://publication/doi/10.1037/h0042519}}
}

@article{Neumann:2012ik,
author = {Neumann, Lukas and Matas, Jiri},
title = {{Real-time scene text localization and recognition}},
journal = {2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2012},
pages = {3538--3545},
publisher = {IEEE},
doi = {10.1109/CVPR.2012.6248097},
isbn = {978-1-4673-1226-4},
issn = {1063-6919},
language = {English},
read = {Yes},
rating = {0},
date-added = {2016-03-02T09:17:45GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {An end-to-end real-time scene text localization and recognition method is presented. The real-time performance is achieved by posing the character detection problem as an efficient sequential selection from the set of Extremal Regions (ERs). The ER detector is robust to blur, illumination, color and texture variation and handles low-contrast text. In the first classification stage, the probability of each ER being a character is estimated using novel features calculated with O(1) complexity per region tested. Only ERs with locally maximal probability are selected for the second stage, where the classification is improved using more computationally expensive features. A highly efficient exhaustive search with feedback loops is then applied to group ERs into words and to select the most probable character segmentation. Finally, text is recognized in an OCR stage trained using synthetic fonts. The method was evaluated on two public datasets. On the ICDAR 2011 dataset, the method achieves state-of-the-art text localization results amongst published methods and it is the first one to report results for end-to-end text recognition. On the more challenging Street View Text dataset, the method achieves state-of-the-art recall. The robustness of the proposed method against noise and low contrast of characters is demonstrated by {\&}{\#}x201C;false positives{\&}{\#}x201D; caused by detected watermark text in the dataset.},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6248097},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/9C/9C016B0D-1AA8-4785-8E7A-E2F38D507485.pdf},
file = {{9C016B0D-1AA8-4785-8E7A-E2F38D507485.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/9C/9C016B0D-1AA8-4785-8E7A-E2F38D507485.pdf:application/pdf;9C016B0D-1AA8-4785-8E7A-E2F38D507485.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/9C/9C016B0D-1AA8-4785-8E7A-E2F38D507485.pdf:application/pdf}},
uri = {\url{papers3://publication/doi/10.1109/CVPR.2012.6248097}}
}

@article{Neumann:2011dy,
author = {Neumann, Lukas and Matas, Jiri},
title = {{Text Localization in Real-World Images Using Efficiently Pruned Exhaustive Search}},
journal = {Document Analysis and Recognition ( {\ldots}},
year = {2011},
pages = {687--691},
publisher = {IEEE},
doi = {10.1109/ICDAR.2011.144},
isbn = {978-1-4577-1350-7},
issn = {1520-5363},
language = {English},
read = {Yes},
rating = {0},
date-added = {2016-04-23T09:59:29GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {An efficient method for text localization and recognition in real-world images is proposed. Thanks to effective pruning, it is able to exhaustively search the space of all character sequences in real time (200ms on a 640 {\&}{\#}x00D7; 480 image). The method exploits higher-order properties of text such as word text lines. We demonstrate that the grouping stage plays a key role in the text localization performance and that a robust and precise grouping stage is able to compensate errors of the character detector. The method includes a novel selector of Maximally Stable Extremal Regions (MSER) which exploits region topology. Experimental validation shows that 95.7% characters in the ICDAR dataset are detected using the novel selector of MSERs with a low sensitivity threshold. The proposed method was evaluated on the standard ICDAR 2003 dataset where it achieved state-of-the-art results in both text localization and recognition.},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6065399},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/77/77B54A60-1506-4753-8F74-AAD83930B476.pdf},
file = {{77B54A60-1506-4753-8F74-AAD83930B476.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/77/77B54A60-1506-4753-8F74-AAD83930B476.pdf:application/pdf}},
uri = {\url{papers3://publication/doi/10.1109/ICDAR.2011.144}}
}

@inproceedings{Girshick:2014jx,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
title = {{Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}},
booktitle = {Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on},
year = {2014},
pages = {580--587},
publisher = {IEEE},
doi = {10.1109/CVPR.2014.81},
isbn = {978-1-4799-5118-5},
read = {Yes},
rating = {0},
date-added = {2016-02-23T11:11:11GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909475},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/5E/5EFBC2D4-8834-4504-9D50-F910FE04929E.pdf},
file = {{5EFBC2D4-8834-4504-9D50-F910FE04929E.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/5E/5EFBC2D4-8834-4504-9D50-F910FE04929E.pdf:application/pdf;5EFBC2D4-8834-4504-9D50-F910FE04929E.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/5E/5EFBC2D4-8834-4504-9D50-F910FE04929E.pdf:application/pdf}},
uri = {\url{papers3://publication/doi/10.1109/CVPR.2014.81}}
}

@article{Ren:2015ug,
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross B and 0001, Jian Sun},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.}},
journal = {NIPS},
year = {2015},
pages = {91--99},
read = {Yes},
rating = {0},
date-added = {2016-02-23T11:14:54GMT},
date-modified = {2016-05-18T12:40:41GMT},
url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/72/72156848-E837-4EA0-955B-7BC9031FF917.pdf},
file = {{72156848-E837-4EA0-955B-7BC9031FF917.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/72/72156848-E837-4EA0-955B-7BC9031FF917.pdf:application/pdf;72156848-E837-4EA0-955B-7BC9031FF917.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/72/72156848-E837-4EA0-955B-7BC9031FF917.pdf:application/pdf}},
uri = {\url{papers3://publication/uuid/AFE9DD65-0E7A-4ED3-8A3E-4B47CC688C23}}
}

@article{He:2015tt,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {{Deep Residual Learning for Image Recognition}},
journal = {arXiv.org},
year = {2015},
eprint = {1512.03385v1},
eprinttype = {arxiv},
eprintclass = {cs.CV},
month = dec,
read = {Yes},
rating = {0},
date-added = {2016-02-11T10:04:35GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
url = {http://arxiv.org/abs/1512.03385v1},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/E6/E62B2FF0-059D-446D-8C9C-8ED8711EFA23.pdf},
file = {{E62B2FF0-059D-446D-8C9C-8ED8711EFA23.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/E6/E62B2FF0-059D-446D-8C9C-8ED8711EFA23.pdf:application/pdf;E62B2FF0-059D-446D-8C9C-8ED8711EFA23.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/E6/E62B2FF0-059D-446D-8C9C-8ED8711EFA23.pdf:application/pdf}},
uri = {\url{papers3://publication/uuid/24EFFE2A-0B42-451F-880F-FD929F3D6E4C}}
}

@article{Kingma:2014us,
author = {Kingma, Diederik and Ba, Jimmy},
title = {{Adam: A Method for Stochastic Optimization}},
journal = {arXiv.org},
year = {2014},
eprint = {1412.6980v8},
eprinttype = {arxiv},
eprintclass = {cs.LG},
month = dec,
read = {Yes},
rating = {0},
date-added = {2016-02-06T09:34:18GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
url = {http://arxiv.org/abs/1412.6980v8},
uri = {\url{papers3://publication/uuid/42DED322-EF05-4630-860B-FD53C8255CC9}}
}

@article{Gou:2014ds,
author = {Gou, C and Wang, K and Li, B and Wang, F Y},
title = {{Vehicle license plate recognition based on class-specific ERs and SaE-ELM}},
journal = {{\ldots} Transportation Systems (ITSC {\ldots}},
year = {2014},
doi = {10.1109/ITSC.2014.6958164},
read = {Yes},
rating = {0},
date-added = {2016-04-05T07:53:37GMT},
date-modified = {2016-05-18T12:40:41GMT},
abstract = {Page 1. Vehicle License Plate Recognition Based on Class-specific ERs and SaE-ELM Chao Gou, Kunfeng Wang, Bo Li, and Fei-yue Wang Abstract{\textemdash}In this paper, an effective approach to vehicle license plate recognition based ... 
},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6958164},
local-url = {file://localhost/Users/Changxu/Dropbox/Library.papers3/Files/B0/B0EF8A66-00DD-4F81-B10C-B631AE133C32.pdf},
file = {{B0EF8A66-00DD-4F81-B10C-B631AE133C32.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/B0/B0EF8A66-00DD-4F81-B10C-B631AE133C32.pdf:application/pdf;B0EF8A66-00DD-4F81-B10C-B631AE133C32.pdf:/Users/Changxu/Dropbox/Library.papers3/Files/B0/B0EF8A66-00DD-4F81-B10C-B631AE133C32.pdf:application/pdf}},
uri = {\url{papers3://publication/doi/10.1109/ITSC.2014.6958164}}
}

